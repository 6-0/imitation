{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of EPIC Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from imitation.rewards.reward_distance.collections import ModelCollection\n",
    "from imitation.rewards.reward_distance.reward_models import (ZeroRewardModel,\n",
    "                                          ConstantRewardModel,\n",
    "                                          GroundTruthRewardModelForHC,\n",
    "                                          RewardModelWrapperForImitation,\n",
    "                                          RandomRewardModel)\n",
    "from imitation.rewards.reward_distance.mujoco_sampler import MujocoTransitionSampler\n",
    "from imitation.rewards.reward_distance.transition_sampler import UniformlyRandomActionSampler\n",
    "from imitation.rewards.reward_distance.epic import EPIC\n",
    "from imitation.rewards.reward_distance.distances import compute_distance_between_reward_pairs\n",
    "from imitation.rewards.reward_distance.distances import compute_pearson_distance\n",
    "from imitation.rewards.serialize import load_reward\n",
    "\n",
    "# Give path to appropriate reward directory\n",
    "imitation_reward_model_path = \"/mnt/models/exp-0607-cheetah_train_pc_then_rl_sac/2022-06-07T17:04:38+00:00/EXP-cheetah_train_pc_then_rl_sac/inner_ef0e9_00000_0_normalize_output_layer=<class 'imitation.util.networks.RunningNorm'>,pc_seed=0,named_configs=['dmc_cheetah_run_2022-06-07_17-04-46/output/dmc_cheetah_run/seed/0/checkpoints/final/reward_net.pt\"\n",
    "\n",
    "\n",
    "def get_action_dim(make_env_fn):\n",
    "    \"\"\"An auxillary function that simply returns action dimension\n",
    "    of given mujoco environment\"\"\"\n",
    "    env = make_env_fn()\n",
    "    n = env.action_space.shape[0]\n",
    "    del env\n",
    "    return n\n",
    "\n",
    "def get_data_for_epic(make_env_fn, states_to_sample=5000):\n",
    "    single_action_sampler = UniformlyRandomActionSampler(num_actions=1,\n",
    "                                                  max_magnitude=1,\n",
    "                                                  action_dim=get_action_dim(make_env_fn))\n",
    "    transition_sampler = MujocoTransitionSampler(make_env_fn,\n",
    "                                                 single_action_sampler,\n",
    "                                                 num_workers=2)\n",
    "    env = make_env_fn()\n",
    "    # Lets sample some transitions for computing rewards\n",
    "    init_states = torch.from_numpy(np.concatenate(\n",
    "            [env.observation_space.sample()[None,...] for _ in range(states_to_sample)],\n",
    "            axis=0))\n",
    "    actions, next_states, _ = transition_sampler.sample(init_states)\n",
    "    actions, next_states = torch.squeeze(actions), torch.squeeze(next_states)\n",
    "    return init_states, actions, next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the enviroment to test on\n",
    "env_name = \"HalfCheetah-v3\"\n",
    "# We may need to make this environment several time, so lets make this\n",
    "# into a function call\n",
    "make_env_fn = lambda :gym.make(env_name, exclude_current_positions_from_observation=False)\n",
    "env = make_env_fn()\n",
    "reward_model1 = RandomRewardModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imitation_reward_fn = load_reward(reward_type=\"RewardNet_unshaped\",\n",
    "                                  reward_path=imitation_reward_model_path,\n",
    "                                  venv = make_env_fn())\n",
    "reward_model2 = RewardModelWrapperForImitation(imitation_reward_fn)\n",
    "reward_model3 = RewardModelWrapperForImitation(imitation_reward_fn)\n",
    "ground_truth_rm = GroundTruthRewardModelForHC(make_env_fn)\n",
    "reward_models = ModelCollection(dict(random_model=reward_model1,\n",
    "                                     IL_model1=reward_model2,\n",
    "                                     IL_model2=reward_model3,\n",
    "                                     ground_truth_rm=ground_truth_rm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how the epic should sample transitions\n",
    "action_sampler = UniformlyRandomActionSampler(num_actions=20,\n",
    "                                              max_magnitude=1,\n",
    "                                              action_dim=get_action_dim(make_env_fn))\n",
    "transition_sampler = MujocoTransitionSampler(make_env_fn,\n",
    "                                             action_sampler,\n",
    "                                             num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "init_states, actions, next_states = get_data_for_epic(make_env_fn)\n",
    "\n",
    "rewards = EPIC().compute_canonical_rewards(models=reward_models,\n",
    "                                       states=init_states,\n",
    "                                       actions=actions,\n",
    "                                       next_states=next_states,\n",
    "                                       terminals=torch.zeros_like(init_states),\n",
    "                                       transition_sampler=transition_sampler,\n",
    "                                       discount=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.     0.7011 0.7011 0.7091]\n",
      " [0.7011 0.     0.     0.6122]\n",
      " [0.7011 0.     0.     0.6122]\n",
      " [0.7091 0.6122 0.6122 0.    ]]\n"
     ]
    }
   ],
   "source": [
    "distances = compute_distance_between_reward_pairs(rewards, compute_pearson_distance)\n",
    "print(np.round(distances.distances, 4))\n",
    "\n",
    "# Optionally you can save a heatmap visualization of distance matrix\n",
    "distances.visualize(\"distances.png\", title=\"Epic Sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nbreg": {
   "diff_ignore": [
    "/metadata/language_info/version"
   ]
  },
  "vscode": {
   "interpreter": {
    "hash": "93cbf6a0f87404a5f3a57ea094cb4b7c2d5dc7e2fa94d5fcfdf2912aea95fc97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
